"""

Message unit adalah unit dasar dari message dalam langchain. Message unit adalah objek yang merepresentasikan 
sebuah pesan dalam konteks percakapan. Message unit dapat berisi teks, gambar, audio, video, atau bahkan data 
struktur lainnya. Message unit digunakan untuk merepresentasikan pesan dalam konteks percakapan, seperti dalam chatbot, 
sistem dialog, atau aplikasi yang memerlukan interaksi antara pengguna dan sistem.

Message unit memiliki beberapa properti, seperti content, role, dan metadata. Content adalah teks pesan, role adalah 
peran pesan dalam konteks percakapan, dan metadata adalah data tambahan tentang pesan.

Message unit dapat digunakan untuk membuat prompt untuk model, seperti dalam chatbot, sistem dialog, atau aplikasi 
yang memerlukan interaksi antara pengguna dan sistem.

"""



from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

from langchain_openai import ChatOpenAI
from os import getenv
from dotenv import load_dotenv
load_dotenv()
llm = ChatOpenAI(
    api_key=getenv("OPENROUTER_API_KEY"),
    base_url="https://openrouter.ai/api/v1",
    model=getenv("MODEL"),
)

"""
Basic usage adalah cara sederhana untuk menggunakan message unit dalam langchain.
Use text prompts when:
You have a single, standalone request
You don’t need conversation history
You want minimal code complexity
"""

print("==== Basic Usage" * 1)

"Basic Usage"
response = llm.invoke([HumanMessage(content="Hello, how are you?")])
print(response)
print(response.content)


"""
Message prompt adalah cara untuk membuat prompt yang lebih kompleks dengan menggunakan message unit dalam langchain.
Use message prompts when:
Managing multi-turn conversations
Working with multimodal content (images, audio, files)
Including system instructions
"""

print("==== Message Prompt" * 1)

"message prompt"

llm2 = ChatOpenAI(
    api_key=getenv("OPENROUTER_API_KEY"),
    base_url="https://openrouter.ai/api/v1",
    model="nvidia/nemotron-nano-12b-v2-vl:free",
)

messages = [
    SystemMessage("You are a helpful assistant."),
    HumanMessage("What is the meaning of life?"),
    AIMessage("The meaning of life is to find happiness and fulfillment related by meaning of this image."),
]
image_url = "https://cdn.pixabay.com/photo/2017/03/23/16/48/japanese-cherry-blossom-2168858_1280.jpg"
image_message = HumanMessage(content=[
    {"type": "image", "url": image_url},
])
response = llm2.invoke(messages + [image_message])
print(response.content)

"""
Dictionary format adalah cara untuk membuat prompt yang lebih kompleks dengan menggunakan message unit dalam langchain dengan format dictionary.
Use dictionary format when:
System message - Tells the model how to behave and provide context for interactions
Human message - Represents user input and interactions with the model
AI message - Responses generated by the model, including text content, tool calls, and metadata
Tool message - Represents the outputs of tool calls
"""

print("==== Dictionary Format" * 1)

"Dictionary format"

messages = [
    {"role": "system", "content": "You are a poetry expert"},
    {"role": "user", "content": "Write a haiku about spring"},
    {"role": "assistant", "content": "Cherry blossoms bloom..."}
]

"""  File "C:\savepoint\learn2_langchain.py", line 86, in <module>
    cain = messages | llm.invoke(messages)
           ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~
TypeError: unsupported operand type(s) for |: 'list' and 'AIMessage'"""
cain = llm.invoke(messages)
print(cain.content)
print(type(cain.content))

"""
System message adalah pesan yang digunakan untuk memberikan instruksi atau konteks kepada model tentang bagaimana seharusnya berperilaku dalam percakapan.
System message biasanya digunakan untuk mengatur nada, gaya, atau tujuan dari interaksi dengan model. Misalnya, 
system message dapat digunakan untuk memberitahu model bahwa ia harus bertindak sebagai seorang ahli dalam suatu bidang tertentu,
atau bahwa ia harus memberikan jawaban yang singkat dan jelas.

"""
print("==== System Message" * 1)

"System message"


from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
import logging
logging.basicConfig(level=logging.INFO)
system_msg = SystemMessage("""
You are a helpful assistant looking to help teacher create calculus lesson plan. You will be given a topic and you will create a lesson plan for that topic. 
The lesson plan should include the following:
1. Learning objectives: What students should be able to do after the lesson.
2. Materials needed: What materials are needed for the lesson.
3. Introduction: A brief introduction to the topic.
4. Main content: The main content of the lesson, including explanations, examples, and activities.
5. Conclusion: A brief conclusion to summarize the lesson.
""")

messages = [
    system_msg,
    HumanMessage("How to teach limits in calculus?")
]
response = llm.invoke(messages)

logging.info(response.content)

"""
Message metadata adalah data tambahan yang dapat disertakan dalam pesan untuk memberikan informasi lebih lanjut 
tentang pesan tersebut. Metadata dapat digunakan untuk berbagai tujuan, 
seperti memberikan konteks tambahan. "identify different users", "unique identifier for tracing" selain itu metadata juga
dapat digunakan untuk memberikan informasi tentang jenis pesan, seperti apakah pesan tersebut adalah pesan teks, gambar, audio, atau video.
"""
print("==== Message Metadata" * 1)

"Message metadata"

"""AI message adalah pesan yang dihasilkan oleh model sebagai respons terhadap input dari pengguna. 
AI message dapat berisi teks, gambar, audio, video, atau data struktur lainnya yang dihasilkan oleh model
berdasarkan permintaan pengguna. AI message digunakan untuk memberikan respons kepada pengguna dalam konteks percakapan,
seperti dalam chatbot, sistem dialog, atau aplikasi yang memerlukan interaksi antara pengguna dan sistem."""
"AI message"

human_msg = HumanMessage(
    content="Hello! Can you help me with a problem I'm having with my math homework?",
    name="alice",  
    id="msg_123",
    metadata={"text": "konsep dasar limit"} 
)

response = llm.invoke([human_msg])
print(type(human_msg))  # Output: <class 'langchain_core.messages.HumanMessage'>
logging.info(response.content)
print(type(response)) #<class 'langchain_core.messages.ai.AIMessage'>



# Create an AI message manually (e.g., for conversation history)
ai_msg = AIMessage(response.content, name="assistant", id="msg_124", metadata={"text": "konsep dasar limit"})

# Add to conversation history
messages = [
    SystemMessage("You are a helpful assistant"),
    HumanMessage("Can you help me?"),
    ai_msg,  # Insert as if it came from the model
    HumanMessage("Explain that concept further."),
]

response = llm.invoke(messages)
logging.info(response.content)

"""
Tools calls adalah fitur yang memungkinkan model untuk memanggil fungsi atau alat eksternal selama proses generasi respons.
Tool calls memungkinkan model untuk melakukan tindakan tertentu, seperti mengambil data dari database, melakukan perhitungan
"""

print("==== Tool Calls" * 1)

"Tool Calls"

from langchain_openai import ChatOpenAI
from os import getenv



def calc(expression: str) -> float:
    try:
        return eval(expression)
    except Exception as e:
        return f"Error: {e}"


llmv20 = ChatOpenAI(
    api_key=getenv("OPENROUTER_API_KEY"),
    base_url="https://openrouter.ai/api/v1",
    model="stepfun/step-3.5-flash:free",
)

model_with_tools = llmv20.bind_tools([calc])

response = model_with_tools.invoke(
    "What is 10 x 200? Use the calc tool to compute the answer."
)

for tool_call in response.tool_calls:
    print(f"Tool: {tool_call['name']}")
    print(f"Args: {tool_call['args']}")
    print(f"ID: {tool_call['id']}")

if response.tool_calls:
    tool_call = response.tool_calls[0]
    tool_args = tool_call["args"]
    expression = tool_args["expression"]
    result = calc(expression)
    print(f"Tool result: {result}")

"""
Token usage adalah jumlah token yang digunakan dalam proses generasi respons oleh model. 
Token adalah unit dasar dari teks yang digunakan oleh model untuk memproses dan menghasilkan respons. 
Token dapat berupa kata, karakter, atau bahkan sub-kata tergantung pada tokenisasi yang digunakan oleh model. 
Token usage penting untuk dipantau karena banyak model memiliki batasan jumlah token yang dapat diproses 
dalam satu permintaan, dan penggunaan token yang berlebihan dapat menyebabkan kegagalan dalam menghasilkan respons baik karena melebihi batas token atau karena
 biaya yang terkait dengan penggunaan token yang tinggi.
"""
print("====Token Usage" * 1)
"Token usage"
response = llm.invoke(
    "Hello!"
)
print(response.content)
print(response.usage_metadata["total_tokens"])
print(response.usage_metadata["input_token_details"])
print(response.usage_metadata["output_token_details"])
print(response.usage_metadata["input_tokens"])
print(response.usage_metadata["output_tokens"])
"""
Streaming and chunks adalah fitur yang memungkinkan model untuk menghasilkan respons secara bertahap atau dalam 
potongan-potongan kecil, daripada menghasilkan seluruh respons sekaligus.
Streaming memungkinkan model untuk mengirimkan bagian-bagian respons saat mereka dihasilkan, 
yang dapat meningkatkan pengalaman pengguna dengan memberikan respons lebih cepat.
Chunks memungkinkan model untuk membagi respons menjadi bagian-bagian yang lebih kecil, 
yang dapat membantu dalam mengelola respons yang panjang memberikan respons yang lebih cepat dan memungkinkan
pengguna untuk mulai membaca respons sebelum seluruhnya dihasilkan.
"""
print("==== Streaming and Chunks" * 1)

def chunks(llm, prompt):
    chunks = []
    full_message = None
    for chunk in llm.stream(prompt):
        chunks.append(chunk)
        print(chunk.text)
        full_message = chunk if full_message is None else full_message + chunk
    print("Full message:", full_message.text)

chunks(llm, "Hi")

print("==== Basic tool definition" * 1)

"Basic tool definition"
from langchain_core.tools import tool

@tool
def search_database(query: str, limit: int = 10) -> str:
    """Search the customer database for records matching the query.

    Args:
        query: Search terms to look for
        limit: Maximum number of results to return
    """

    return f"Found {limit} results for '{query}'"

print(search_database.invoke({"query": "Riwayat Transaksi Rizky", "limit": 5}))

# print("==== Custom Tool name" * 1)

# "Custom tool name"
# @tool("web_search")  # Custom name
# def search(query: str) -> str:
#     return f"Results for: {query}"
# print(search.invoke({"query": "What is the capital of France?"}))
# print(search.name)  # web_search

from pydantic import BaseModel, Field
from typing import Literal


"""Advanced schema definition adalah cara untuk mendefinisikan skema input yang lebih 
kompleks untuk alat dalam langchain menggunakan Pydantic. """

print("==== Basic Advanced Schema Definition" * 1)

class WeatherInput(BaseModel):
    """Input for weather queries."""
    location: str = Field(description="City name or coordinates")
    units: Literal["celsius", "fahrenheit"] = Field(
        default="celsius",
        description="Temperature unit preference"
    )
    include_forecast: bool = Field(
        default=False,
        description="Include 5-day forecast"
    )

@tool(args_schema=WeatherInput)
def get_weather(location: str, units: str = "celsius", include_forecast: bool = False) -> str:
    """Get current weather and optional forecast."""
    temp = 22 if units == "celsius" else 72
    result = f"Current weather in {location}: {temp} degrees {units[0].upper()}"
    if include_forecast:
        result += "\nNext 5 days: Sunny"
    return result

print(get_weather.invoke({"location": "Jakarta", "units": "celsius", "include_forecast": True}))

"""Short-term memory (State) adalah kemampuan model untuk menyimpan dan mengingat informasi selama percakapan berlangsung. Short-term memory memungkinkan model untuk mempertahankan konteks percakapan,
mengingat informasi yang telah diberikan sebelumnya, dan menggunakan informasi tersebut untuk memberikan respons yang lebih relevan dan koheren. Short-term memory biasanya digunakan dalam chatbot, sistem dialog, 
atau aplikasi yang memerlukan interaksi antara pengguna dan sistem, di mana model perlu mengingat informasi dari percakapan sebelumnya untuk memberikan respons yang lebih baik.
perbedaan dengan long-term memory adalah short-term memory hanya menyimpan informasi selama percakapan berlangsung, 
sedangkan long-term memory dapat menyimpan informasi untuk jangka waktu yang lebih lama, bahkan setelah percakapan selesai.
"""

print("==== Short-term Memory (State)" * 1)

from langchain_core.tools import tool
from langchain_core.messages import HumanMessage


# Tool to get all user messages
@tool
def get_all_user_messages(runtime: dict) -> list:
    """Get all user messages' content as a list."""
    messages = runtime["messages"]
    return [m.content for m in messages if isinstance(m, HumanMessage)]

# Tool to get the last user message
@tool
def get_last_user_message(runtime: dict) -> str:
    """Get the most recent message from the user."""
    messages = runtime["messages"]
    for message in reversed(messages):
        if isinstance(message, HumanMessage):
            return message.content
    return "No user messages found"

# Access custom state fields
@tool
def get_user_preference(
    pref_name: str,
    runtime: dict
) -> str:
    """Get a user preference value."""
    preferences = runtime.get("user_preferences", {})
    return preferences.get(pref_name, "Not set")


user_messages = [
    HumanMessage(content="Hello!"),
    HumanMessage(content="What is the weather like today?"),
    HumanMessage(content="Can you help me with my homework?"),
]
print(get_last_user_message.invoke({"runtime": {"messages": user_messages}}))
print(get_all_user_messages.invoke({"runtime": {"messages": user_messages}}))


print(get_user_preference.invoke({"pref_name": "language", "runtime": {"user_preferences": {"language": "English"}}}))


"Update state Use Command to update the agent’s state. This is useful for tools that need to update custom state fields"
"""
Context Context provides immutable configuration data that is passed at invocation time. Use it for user IDs, session details, or 
application-specific settings that shouldn’t change during a conversation."""

print("==== Basic Usage of Context" * 1)

from dataclasses import dataclass
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from langchain_core.tools import tool


USER_DATABASE = {
    "user123": {
        "name": "Alice Johnson",
        "account_type": "Premium",
        "balance": 5000,
        "email": "alice@example.com"
    },
    "user456": {
        "name": "Bob Smith",
        "account_type": "Standard",
        "balance": 1200,
        "email": "bob@example.com"
    }
}

@dataclass
class UserContext:
    user_id: str


@tool
def get_account_info(context: UserContext) -> str:
    """Get the current user's account information."""
    
    user_id = context.user_id

    if user_id in USER_DATABASE:
        user = USER_DATABASE[user_id]
        return (
            f"Account holder: {user['name']}\n"
            f"Type: {user['account_type']}\n"
            f"Balance: ${user['balance']}"
        )
    
    return "User not found"


# model = ChatOpenAI(model="gpt-4.1")

agent = create_agent(
    llm,
    tools=[get_account_info],
    context_schema=UserContext,
    system_prompt="You are a financial assistant."
)

result = agent.invoke(
    {"messages": [{"role": "user", "content": "What's my current balance?"}]},
    context=UserContext(user_id="user123")
)

print(result["messages"][-1].content)